{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Instacart.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl7F0OCGLSLW"
      },
      "source": [
        "import numpy as np \n",
        "from scipy import stats\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import KFold \n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmBFbbta4lXQ"
      },
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KkkXm-c0__R"
      },
      "source": [
        "# folder_path = '/content/drive/MyDrive/python_data/kaggle/instacart/data/'\n",
        "# files = ['aisles.csv', 'departments.csv', 'order_products__prior.csv', 'order_products__train.csv', 'orders.csv', 'products.csv']\n",
        "# aisle = pd.read_csv(folder_path+files[0])\n",
        "# dep = pd.read_csv(folder_path+files[1])\n",
        "# prior = pd.read_csv(folder_path+files[2])\n",
        "# train = pd.read_csv(folder_path+files[3])\n",
        "# orders = pd.read_csv(folder_path+files[4])\n",
        "# products = pd.read_csv(folder_path+files[5])\n",
        "# test = orders[orders['eval_set'] == 'test']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2mADG9Z1ri8"
      },
      "source": [
        "# print(len(products))\n",
        "# products.head(2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SgK4E2K3bsz"
      },
      "source": [
        "# orders.head(2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSee2qSrE9SF"
      },
      "source": [
        "# prior.head(2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F2GtewegNZGw",
        "outputId": "e7071a45-81e4-4f11-e7da-028a3944b1df"
      },
      "source": [
        "'''train和test資料的user_id不重複'''\n",
        "\n",
        "# len(set(orders[orders['eval_set']=='train']['user_id']).intersection(set(orders[orders['eval_set']=='test']['user_id'])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'train和test資料的user_id不重複'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gekO9UW8X2HV"
      },
      "source": [
        "# Preprocessing\n",
        "* 將prior資訊 agg: df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG_P3wlGLpPo"
      },
      "source": [
        "## Hour2cat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NZW635iKn4R"
      },
      "source": [
        "def hour2cat(x):\n",
        "  if (x>=6) & (x<= 12): #早上\n",
        "    y= 0\n",
        "  elif (x>12) & (x< 18): #下午\n",
        "    y= 1\n",
        "  else:\n",
        "    y= 2 #晚上\n",
        "  return y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxMo7Tb3J0UP"
      },
      "source": [
        "# orders['order_hour_of_day'] = orders['order_hour_of_day'].apply(lambda x: hour2cat(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIxsdp2budOC"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vdTRtBuuhsz"
      },
      "source": [
        "def get_data(train_bool= True):\n",
        "  \n",
        "  prior_order = orders[orders['eval_set']=='prior'].drop(columns= ['eval_set'], axis= 1)\n",
        "\n",
        "  if train_bool:\n",
        "    \n",
        "    train_order = orders[orders['eval_set']=='train'].drop(columns= ['eval_set'], axis= 1)\n",
        "    train_order.columns = ['train_'+col for col in train_order.columns]\n",
        "    prior_order.columns = ['prior_'+col for col in prior_order.columns]\n",
        "    train_prior = pd.merge(train_order, prior_order, left_on= 'train_user_id', right_on='prior_user_id')\n",
        "    train_X = pd.merge(train_prior, prior, left_on= 'prior_order_id', right_on= 'order_id')\n",
        "    train_X = pd.merge(train_X, products, left_on= 'product_id', right_on= 'product_id')\n",
        "\n",
        "    '''針對歷史order資訊做groupby'''\n",
        "    train_cols = ['train_order_id', 'train_user_id', 'train_order_dow', 'train_order_hour_of_day', 'train_days_since_prior_order', 'product_id', 'aisle_id', 'department_id']\n",
        "    grouped_train_X = train_X.groupby(train_cols, dropna=False).agg(\n",
        "      prior_order_count= ('prior_order_id', 'count'), \n",
        "      prior_dow_mode= ('prior_order_dow', lambda x: Counter(x).most_common(1)[0][0]), \n",
        "      prior_hod_mode= ('prior_order_hour_of_day', lambda x: Counter(x).most_common(1)[0][0]), \n",
        "      prior_dspo_mean= ('prior_days_since_prior_order', 'mean'),\n",
        "      prior_dspo_var= ('prior_days_since_prior_order', 'var')\n",
        "      ).reset_index().fillna(0)\n",
        "\n",
        "    '''user feature with dep & aisle'''\n",
        "    train_user_dep_ratio= train_X.groupby(['train_user_id', 'department_id'])['department_id'].count()/train_X.groupby(['train_user_id'])['department_id'].count()\n",
        "    train_user_aisle_ratio= train_X.groupby(['train_user_id', 'aisle_id'])['aisle_id'].count()/train_X.groupby(['train_user_id'])['aisle_id'].count()\n",
        "    grouped_train_X['user_dep_ratio'] = train_user_dep_ratio[pd.MultiIndex.from_frame(grouped_train_X[['train_user_id', 'department_id']])].values\n",
        "    grouped_train_X['user_aisle_ratio'] = train_user_aisle_ratio[pd.MultiIndex.from_frame(grouped_train_X[['train_user_id', 'aisle_id']])].values\n",
        "\n",
        "    '''Train x合併y'''\n",
        "    full_X = pd.merge(grouped_train_X, train.rename(columns={'reordered':'label'}), how= 'left', left_on=['train_order_id','product_id'], right_on= ['order_id', 'product_id'])\n",
        "\n",
        "    '''drop不需要的columns'''\n",
        "    drop_cols= ['train_user_id', 'train_order_id', 'order_id', 'add_to_cart_order']\n",
        "    train_X = full_X.drop(columns=drop_cols, axis= 1).fillna(0).iloc[:, :-1]\n",
        "    train_y = full_X.drop(columns=drop_cols, axis= 1).fillna(0).iloc[:, -1]\n",
        "\n",
        "    return train_X, train_y\n",
        "  \n",
        "  else:\n",
        "    \n",
        "    test_order = orders[orders['eval_set']=='test'].drop(columns= ['eval_set'], axis= 1)\n",
        "    prior_order.columns = ['prior_'+col for col in prior_order.columns]\n",
        "    test_order.columns = ['test_'+col for col in test_order.columns]\n",
        "    test_prior = pd.merge(test_order, prior_order, left_on= 'test_user_id', right_on='prior_user_id')\n",
        "    test_X = pd.merge(test_prior, prior, left_on= 'prior_order_id', right_on= 'order_id')\n",
        "    test_X = pd.merge(test_X, products, left_on= 'product_id', right_on= 'product_id')\n",
        "\n",
        "    '''針對歷史order資訊做groupby'''\n",
        "    \n",
        "    test_cols= ['test_order_id', 'test_user_id', 'test_order_dow', 'test_order_hour_of_day', 'test_days_since_prior_order', 'product_id', 'aisle_id', 'department_id']\n",
        "    grouped_test_X = test_X.groupby(test_cols, dropna=False).agg(\n",
        "        prior_order_count= ('prior_order_id', 'count'), \n",
        "        prior_dow_mode= ('prior_order_dow', lambda x: Counter(x).most_common(1)[0][0]), \n",
        "        prior_hod_mode= ('prior_order_hour_of_day', lambda x: Counter(x).most_common(1)[0][0]), \n",
        "        prior_dspo_mean= ('prior_days_since_prior_order', 'mean'),\n",
        "        prior_dspo_var= ('prior_days_since_prior_order', 'var')\n",
        "        ).reset_index().fillna(0)\n",
        "\n",
        "    test_order_id = test_X['test_order_id'].values\n",
        "\n",
        "    '''user feature with dep & aisle'''\n",
        "    test_user_dep_ratio= test_X.groupby(['test_user_id', 'department_id'])['department_id'].count()/test_X.groupby(['test_user_id'])['department_id'].count()\n",
        "    test_user_aisle_ratio= test_X.groupby(['test_user_id', 'aisle_id'])['aisle_id'].count()/test_X.groupby(['test_user_id'])['aisle_id'].count()\n",
        "\n",
        "    grouped_test_X['user_dep_ratio'] = test_user_dep_ratio[pd.MultiIndex.from_frame(grouped_test_X[['test_user_id', 'department_id']])].values\n",
        "    grouped_test_X['user_aisle_ratio'] = test_user_aisle_ratio[pd.MultiIndex.from_frame(grouped_test_X[['test_user_id', 'aisle_id']])].values\n",
        "    test_X = grouped_test_X.drop(columns=['test_user_id', 'test_order_id'], axis= 1).fillna(0)\n",
        "    return test_X, test_order_id"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zYwO50uWe1s"
      },
      "source": [
        "## 觀察\n",
        "* 對於同個train order id且相同product，可能會有許多不同prior order id的訊息，即相同的商品可能買過很多次，要如何組成一組feature？\n",
        "* 平均每個product的prior order數: 2.44\n",
        "* 最大每個product的prior order數: 99"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smApAz4dcJpQ"
      },
      "source": [
        "# avg_n_prior_order = np.mean(train_X.groupby(['train_order_id', 'product_id'])['prior_order_id'].count())\n",
        "# max_n_prior_order = np.max(train_X.groupby(['train_order_id', 'product_id'])['prior_order_id'].count())\n",
        "# print(f'平均每個product的prior order數: {round(avg_n_prior_order, 2)}')\n",
        "# print(f'最大每個product的prior order數: {round(max_n_prior_order, 2)}')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hwoDzu2ofvWQ",
        "outputId": "8d3b9eda-acdc-44d3-815e-8de1953a3316"
      },
      "source": [
        "'''\n",
        "prior_order_dow: mode\n",
        "prior_order_hour_of_day: mode\n",
        "prior_days_since_prior_order: mean, var\n",
        "user_dep: ratio \n",
        "user_aisle: ratio  \n",
        "'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprior_order_dow: mode\\nprior_order_hour_of_day: mode\\nprior_days_since_prior_order: mean, var\\nuser_dep: ratio \\nuser_aisle: ratio  \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbBowHf9fSzK"
      },
      "source": [
        "# grouped_train_X.head(3)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agYZzw0orkY7"
      },
      "source": [
        "# print(len(grouped_train_X))\n",
        "# print(len(test_X))\n",
        "# grouped_train_X.head(3)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpfcskDAa5gE"
      },
      "source": [
        "## 產生Y \n",
        "* by merging grouped X and train data\n",
        "* 有reordered(y=1)僅占10%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTH8qJYlxjIj"
      },
      "source": [
        "# full_X = pd.merge(grouped_train_X, train.rename(columns={'reordered':'label'}), how= 'left', left_on=['train_order_id','product_id'], right_on= ['order_id', 'product_id'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dDbe563ySIS"
      },
      "source": [
        "# drop_cols= ['train_user_id', 'train_order_id', 'order_id', 'add_to_cart_order']\n",
        "# train_X = full_X.drop(columns=drop_cols, axis= 1).fillna(0).iloc[:, :-1]\n",
        "# train_y = full_X.drop(columns=drop_cols, axis= 1).fillna(0).iloc[:, -1]\n",
        "# test_X = test_X.drop(columns=['test_user_id', 'test_order_id'], axis= 1).fillna(0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP1bTyeLzqR4"
      },
      "source": [
        "# train_y.value_counts()*100/len(train_y)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCvLgbyj2IFB"
      },
      "source": [
        "# OneHotEncoding & load data\n",
        "* 先fit好 encoder, 在batch training時將cat data做transform即可，降低記憶體使用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tmx2bsIEv6Bq",
        "outputId": "3354f3b9-15df-441f-e4cd-252cfd59a407"
      },
      "source": [
        "'''helper generator'''\n",
        "# train_X, train_y= get_data(train_bool=True)\n",
        "# test_X, test_order_id= get_data(train_bool=False)\n",
        "\n",
        "'''load pkl from data'''\n",
        "train_X= pd.read_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_X.pkl')\n",
        "train_y= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_y.pkl', allow_pickle=True) # Series\n",
        "test_X= pd.read_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_X.pkl')\n",
        "\n",
        "'''save test_X as dataframe for refering user_id and products'''\n",
        "# train_X.to_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_X.pkl')\n",
        "# train_y.to_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_y.pkl')\n",
        "# test_X.to_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_X.pkl')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'save test_X as dataframe for refering user_id and products'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MihoqQm2OMP"
      },
      "source": [
        "train_dense_cols= ['train_days_since_prior_order', 'prior_order_count', 'prior_dspo_mean', 'prior_dspo_var', 'user_dep_ratio', 'user_aisle_ratio']\n",
        "test_dense_cols= ['test_days_since_prior_order', 'prior_order_count', 'prior_dspo_mean', 'prior_dspo_var', 'user_dep_ratio', 'user_aisle_ratio']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJBx-dSz3w4r"
      },
      "source": [
        "train_X_cat= train_X.drop(train_dense_cols, axis= 1).to_numpy()\n",
        "train_X_dense= train_X[train_dense_cols].to_numpy()\n",
        "train_y= train_y.to_numpy()\n",
        "\n",
        "test_X_cat= test_X.drop(test_dense_cols, axis= 1).to_numpy()\n",
        "test_X_dense= test_X[test_dense_cols].to_numpy()\n",
        "\n",
        "fields = [len(np.unique(train_X_cat[:, i])) for i in range(train_X_cat.shape[1])] + [len(train_dense_cols)]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXkInhvm8iYI"
      },
      "source": [
        "# np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_X_cat.npy', train_X_cat)\n",
        "# np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_X_dense.npy', train_X_dense)\n",
        "# np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_y.npy', train_y)\n",
        "# np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_X_cat.npy', test_X_cat)\n",
        "# np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_X_dense.npy', test_X_dense)\n",
        "# np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/fields.npy', fields)\n",
        "# np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_order_id.npy', test_order_id)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVxIJEz29Fyu"
      },
      "source": [
        "# train_X_cat= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_X_cat.npy')\n",
        "# train_X_dense= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_X_dense.npy')\n",
        "# train_y= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_y.npy')\n",
        "# test_X_cat= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_X_cat.npy')\n",
        "# test_X_dense= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_X_dense.npy')\n",
        "# fields= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/fields.npy')\n",
        "# test_order_id= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_order_id.npy')\n",
        "# test_X= pd.read_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_X.pkl')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5LisUdJmdvV"
      },
      "source": [
        "encoder= OneHotEncoder(sparse= False, handle_unknown='ignore')\n",
        "enc_fitted= encoder.fit(train_X_cat)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO_og5E8M3jW"
      },
      "source": [
        "# batch= train_X_cat[:32]\n",
        "# enc_fitted.transform(batch).shape"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZqelqsdCuAa"
      },
      "source": [
        "# Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qap9YHPSCwYL"
      },
      "source": [
        "class custom_dataset(Dataset):\n",
        "  def __init__(self, X_cat, X_dense, y= None, if_y= False):\n",
        "    self.X_cat = torch.tensor(X_cat, dtype= torch.float)\n",
        "    self.X_dense = torch.tensor(X_dense, dtype= torch.float)\n",
        "    self.if_y= if_y\n",
        "    if if_y:\n",
        "      self.y = torch.tensor(y, dtype= torch.float)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.X_cat)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if self.if_y:\n",
        "      return self.X_cat[idx], self.X_dense[idx], self.y[idx]\n",
        "    else:\n",
        "      return self.X_cat[idx], self.X_dense[idx]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_e_fz12FcIf"
      },
      "source": [
        "# Model \n",
        "* Evaluation: mean F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLRi_-S9FcPY"
      },
      "source": [
        "class DeepFM(nn.Module):\n",
        "  def __init__(self, fields, k= 5, hidden_dims= [16, 16], dropout= 0.2, n_class= 1):\n",
        "    super(DeepFM, self).__init__()\n",
        "    self.fields = fields \n",
        "    self.k = k \n",
        "    self.hidden_dims = hidden_dims\n",
        "\n",
        "    \"\"\"Linear\"\"\"\n",
        "    d = sum(fields)\n",
        "    self.linear = nn.Linear(d, n_class, bias= False)\n",
        "\n",
        "    \"\"\"FM\"\"\"\n",
        "    # self.FM_w = nn.Linear(1, n_class)\n",
        "    self.embedding_ws = nn.ModuleList()\n",
        "    for i in fields:\n",
        "      self.embedding_ws.append(nn.Linear(i, k, bias= False))\n",
        "    \n",
        "    \"\"\"DNN\"\"\"\n",
        "    layers = []\n",
        "    input_dim = k * len(fields)\n",
        "\n",
        "    for hidden_dim in hidden_dims:\n",
        "      layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "      layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Dropout(p=dropout))\n",
        "      input_dim = hidden_dim\n",
        "    \n",
        "    layers.append(nn.Linear(hidden_dims[-1], n_class))\n",
        "    self.dnn = nn.Sequential(*layers)\n",
        "\n",
        "  def Dense_Embedding(self, X):\n",
        "    es = []\n",
        "    start= 0\n",
        "    for i, field in enumerate(self.fields):\n",
        "      ei = self.embedding_ws[i](X[:, start:start+field]).unsqueeze(dim= 1) # ei: [n, 1, k]\n",
        "      # ei = torch.matmul(X[:, start:start+field], self.embedding_ws[i]).unsqueeze(dim= 1) # ei: [n, 1, k]\n",
        "      start += field\n",
        "      es.append(ei)\n",
        "\n",
        "    return torch.cat(es, dim= 1) # [n, n_fields, k]  \n",
        "\n",
        "  \n",
        "  def FM(self, X):\n",
        "\n",
        "    sum_of_square = torch.sum(X, dim= 1)**2 #[n, k]\n",
        "    square_of_sum = torch.sum(X**2, dim= 1)\n",
        "    ix = sum_of_square - square_of_sum \n",
        "    FM_out = 0.5 * torch.sum(ix, dim= 1, keepdim= True) # [n, 1] \n",
        "    # return self.FM_w(FM_out)\n",
        "    return FM_out\n",
        "\n",
        "  def DNN(self, X):\n",
        "\n",
        "    X = X.view(-1, self.k * len(self.fields)) # [n, k*n_fields]\n",
        "    X = self.dnn(X)\n",
        "    return X\n",
        "  \n",
        "  def forward(self, X):\n",
        "\n",
        "    dense_X = self.Dense_Embedding(X)\n",
        "    FM_y = self.FM(dense_X)\n",
        "    DNN_y = self.DNN(dense_X)\n",
        "    y = self.linear(X) + FM_y + DNN_y\n",
        "\n",
        "    # return nn.Sigmoid()(y) # BCELoss\n",
        "    return y # nn.BCEWithLogitsLoss(pos_weight=9)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EWbHL5t60e_"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZI3eM2c61TI"
      },
      "source": [
        "batch_size= 512\n",
        "lr = 1e-3\n",
        "n_epoch = 2\n",
        "k = 20\n",
        "p = 0.3\n",
        "hidden_dims = [128, 128]\n",
        "n_class = 1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvS3EOzG65ci"
      },
      "source": [
        "train_dataset = custom_dataset(train_X_cat, train_X_dense, train_y, if_y= True)\n",
        "test_dataset = custom_dataset(test_X_cat, test_X_dense, if_y= False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle= True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle= False, num_workers=2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgGvPoz365ed"
      },
      "source": [
        "kf = KFold(n_splits=5, shuffle= True, random_state=42)\n",
        "model = DeepFM(fields= fields, k= k, hidden_dims= hidden_dims, dropout= p, n_class= n_class).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "# weight= torch.tensor([1, 10], device= device)\n",
        "# criterion = nn.BCELoss(weight= weight, reduction= 'sum')\n",
        "criterion= nn.BCEWithLogitsLoss(pos_weight=torch.tensor(9, device= device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M808Am5V7CoC"
      },
      "source": [
        "## Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "139lTdHJ65gJ"
      },
      "source": [
        "for epoch in range(n_epoch):\n",
        "  model.train()\n",
        "  total_loss= list()\n",
        "  for i, (X_cat, X_dense, y) in enumerate(tqdm(train_loader)):\n",
        "  # for i, (X_cat, X_dense, y) in enumerate(train_loader):\n",
        "    X_cat_onehot = torch.tensor(enc_fitted.transform(X_cat), dtype= torch.float)\n",
        "    X= torch.cat([X_cat_onehot, X_dense], dim= 1).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output= model(X)\n",
        "    loss= criterion(output, y.unsqueeze(dim= 1).to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss.append(loss.item())\n",
        "    # if i == 200:\n",
        "    #   break\n",
        "  print(f'avg loss: {round(np.mean(total_loss), 4)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4U8q4jB65iG"
      },
      "source": [
        "torch.save(model, f'instacart.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq8SB0Akr6_C"
      },
      "source": [
        "!cp 'instacart.pt' '/content/drive/MyDrive/python_data/kaggle/instacart'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnrnaypUyodD"
      },
      "source": [
        "# Create Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNf20I4PuoKx"
      },
      "source": [
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oL_TTXSJhpZ"
      },
      "source": [
        "# model= torch.load('/content/drive/MyDrive/python_data/kaggle/instacart/instacart.pt', map_location=torch.device('cpu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUpmJ6O5yrNO"
      },
      "source": [
        "'''\n",
        "if nn.BCEWithLogitsLoss(pos_weight=9) is used, \n",
        "then output from the model should pass a sigmoid function to represent probability\n",
        "'''\n",
        "# preds= []\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#   for i, (X_cat, X_dense) in enumerate(tqdm(test_loader)):\n",
        "#     X_cat_onehot = torch.tensor(enc_fitted.transform(X_cat), dtype= torch.float)\n",
        "#     X= torch.cat([X_cat_onehot, X_dense], dim= 1).to(device)\n",
        "#     output= model(X)\n",
        "#     output= nn.Sigmoid()(output) # Careful \n",
        "#     preds.extend(output.squeeze(dim=1).detach().cpu().numpy())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1CHnkT90O_w"
      },
      "source": [
        "# preds = np.array(preds)\n",
        "# np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/preds.npy', preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aFLlA1Wi82L"
      },
      "source": [
        "preds= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/preds.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vywKqTPrExas"
      },
      "source": [
        "test_X['order_id']= test_order_id\n",
        "test_X['pred']= preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JCGTUFvWhSQ"
      },
      "source": [
        "# (test_X_cat[:, 2] == -1).any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrNzK87sjL-s"
      },
      "source": [
        "def rule(x):\n",
        "  if x >= 0.5:\n",
        "    return 1 \n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZmJ7eDxWixC"
      },
      "source": [
        "test_X['pred_binary'] = test_X['pred'].apply(rule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHXpoWvri1gx"
      },
      "source": [
        "test_X['pred_binary'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITgeydR5jVTh"
      },
      "source": [
        "submission_dict= {}\n",
        "for i, row in test_X[test_X['pred_binary']==1].iterrows():\n",
        "  order_id = int(row['order_id'])\n",
        "  product_id= int(row['product_id'])\n",
        "  if order_id in submission_dict.keys():\n",
        "    submission_dict[order_id].append(product_id)\n",
        "  else:\n",
        "    submission_dict[order_id] = [product_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcYMf2V5ljNL"
      },
      "source": [
        "for order_id in test_X[test_X['pred_binary']==0]['order_id'].unique():\n",
        "  if order_id in submission_dict.keys():\n",
        "    pass\n",
        "  else:\n",
        "    submission_dict[order_id]= 'None'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGZHxAb0prPZ"
      },
      "source": [
        "submission_dict.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH8NeWnSvLNo"
      },
      "source": [
        "with open('submission.csv', 'w', newline='') as csvfile:\n",
        "  # 建立 CSV 檔寫入器\n",
        "  writer = csv.writer(csvfile, delimiter=',')\n",
        "\n",
        "  # 寫入一列資料\n",
        "  writer.writerow(['order_id', 'products'])\n",
        "\n",
        "  # 寫入另外幾列資料\n",
        "  for key, value in submission_dict.items():\n",
        "    if value == 'None':\n",
        "      writer.writerow([key, 'None'])\n",
        "    else:\n",
        "      value= [str(id) for id in value]\n",
        "      writer.writerow([str(key), ' '.join(value)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSVDIkS3wM_Y"
      },
      "source": [
        "len(submission_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}