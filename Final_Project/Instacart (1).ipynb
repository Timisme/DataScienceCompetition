{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Instacart.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl7F0OCGLSLW"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmBFbbta4lXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7165a7eb-8b83-4317-da12-a89e943e4df2"
      },
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KkkXm-c0__R"
      },
      "source": [
        "# folder_path = '/content/drive/MyDrive/python_data/kaggle/instacart/data/'\n",
        "# files = ['aisles.csv', 'departments.csv', 'order_products__prior.csv', 'order_products__train.csv', 'orders.csv', 'products.csv']\n",
        "# aisle = pd.read_csv(folder_path+files[0])\n",
        "# dep = pd.read_csv(folder_path+files[1])\n",
        "# prior_product = pd.read_csv(folder_path+files[2])\n",
        "# train_product = pd.read_csv(folder_path+files[3])\n",
        "# orders = pd.read_csv(folder_path+files[4])\n",
        "# products = pd.read_csv(folder_path+files[5]).drop(columns= ['product_name'])\n",
        "\n",
        "# print('training size:', len(train_product))\n",
        "# print('reodered %: ', round(100*train_product['reordered'].value_counts()[1]/len(train_product), 1))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sNf9IoEWtfi"
      },
      "source": [
        "# 概念 & Utils\n",
        "* 138萬training data都有其對應訂單資訊 > merge\n",
        "* 每個training data都是對應一個product_id -> 該user對該product_id歷史資訊\n",
        "* 該user對該product_id歷史資訊要前處理-> agg\n",
        "* 歷史資訊要考慮對該product是否有reordered和add_to_cart_order\n",
        "* We have the training data for new orders in order_products__train.csv but only the metadata for the test orders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP3Y_dd_SuCF"
      },
      "source": [
        "def get_data(eval_set= 'train'):\n",
        "\n",
        "  '''prior order所有資訊'''\n",
        "  order_info_prior = orders[orders['eval_set']== 'prior']\n",
        "  prior_info = pd.merge(prior_product, order_info_prior, on= 'order_id', how= 'left').drop(columns= ['eval_set', 'order_id']) #每個(user_id, product_id)可能不只一個\n",
        "  prior_info.rename(columns= {'reordered':'reordered_prior'}, inplace= True)\n",
        "  # prior_info.head(3)\n",
        "  '''order now資訊'''\n",
        "  order_info_now = orders[orders['eval_set']== eval_set].drop(columns= ['eval_set'])\n",
        "\n",
        "  if eval_set == 'train':\n",
        "    order_info_now = pd.merge(train_product.drop(columns=['add_to_cart_order']), order_info_now, on= 'order_id', how= 'left')\n",
        "    print(order_info_now.head(1))\n",
        "    basic_cols = ['order_id', \n",
        "            'user_id', \n",
        "            'product_id', \n",
        "            'reordered', \n",
        "            'order_number_now', \n",
        "            'order_dow_now', \n",
        "            'order_hour_of_day_now', \n",
        "            'days_since_prior_order_now']\n",
        "    X = pd.merge(order_info_now, prior_info, on= ['user_id', 'product_id'], how= 'left', suffixes= ['_now', '_prior']).fillna(0) # 全部基本資訊\n",
        "  else:\n",
        "    '''user和prior_info 合併'''\n",
        "\n",
        "    basic_cols = ['order_id', \n",
        "            'user_id', \n",
        "            'product_id', \n",
        "            'order_number_now', \n",
        "            'order_dow_now', \n",
        "            'order_hour_of_day_now', \n",
        "            'days_since_prior_order_now']\n",
        "    X = pd.merge(order_info_now, prior_info, on= 'user_id', how= 'left', suffixes= ['_now', '_prior']).fillna(0) # 全部基本資訊\n",
        "\n",
        "  print('group前', X.head(3))\n",
        "  '''same for train and test'''\n",
        "  X = X.groupby(basic_cols).agg(add2cart_mode= ('add_to_cart_order', lambda x: Counter(x).most_common(1)[0][0]),\n",
        "              reorder_max= ('reordered_prior', 'max'), \n",
        "              order_number_max= ('order_number_prior', 'max'),\n",
        "              order_dow_mode= ('order_dow_prior', lambda x: Counter(x).most_common(1)[0][0]),\n",
        "              order_hod_mode= ('order_hour_of_day_prior', lambda x: Counter(x).most_common(1)[0][0]), # transformed already to 1, 2, 3\n",
        "              days_since_mean= ('days_since_prior_order_prior', 'mean'),\n",
        "              days_since_std= ('days_since_prior_order_prior', 'std'),\n",
        "              order_count= ('order_number_prior', 'size')\n",
        "              ).reset_index().fillna(0)\n",
        "  print('group後', X.head(3))\n",
        "\n",
        "  '''加入product info以及user_product_feature'''\n",
        "  X = pd.merge(X, products, on='product_id', how= 'left')\n",
        "  user_dep_ratio= X.groupby(['user_id', 'department_id'])['department_id'].count()/X.groupby(['user_id'])['department_id'].count()\n",
        "  user_aisle_ratio= X.groupby(['user_id', 'aisle_id'])['aisle_id'].count()/X.groupby(['user_id'])['aisle_id'].count()\n",
        "  X['user_dep_ratio'] = user_dep_ratio[pd.MultiIndex.from_frame(X[['user_id', 'department_id']])].values\n",
        "  X['user_aisle_ratio'] = user_aisle_ratio[pd.MultiIndex.from_frame(X[['user_id', 'aisle_id']])].values\n",
        "  \n",
        "  '''drop不需要的columns'''\n",
        "  if eval_set == 'train':\n",
        "    # train_y = X['reordered']\n",
        "    # X.drop(columns=['order_id', 'user_id', 'reordered'], inplace=True)\n",
        "    return X\n",
        "  else:\n",
        "    # order_ids = X['order_id']\n",
        "    # X.drop(columns=['order_id', 'user_id'], inplace=True)\n",
        "    return X"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlazedIEAAHh"
      },
      "source": [
        "def hour2cat(x):\n",
        "  if (x>=6) & (x<= 12): #早上\n",
        "    y= 0\n",
        "  elif (x>12) & (x< 18): #下午\n",
        "    y= 1\n",
        "  else:\n",
        "    y= 2 #晚上\n",
        "  return y"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F2GtewegNZGw",
        "outputId": "dcb23bcd-dc20-4025-b604-d1e64efa85cf"
      },
      "source": [
        "'''train和test資料的user_id不重複'''\n",
        "\n",
        "# len(set(orders[orders['eval_set']=='train']['user_id']).intersection(set(orders[orders['eval_set']=='test']['user_id'])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'train和test資料的user_id不重複'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zYwO50uWe1s"
      },
      "source": [
        "## 觀察\n",
        "* 對於同個train order id且相同product，可能會有許多不同prior order id的訊息，即相同的商品可能買過很多次，要如何組成一組feature？\n",
        "* 平均每個product的prior order數: 2.44\n",
        "* 最大每個product的prior order數: 99\n",
        "* 有reordered(y=1)僅占10%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smApAz4dcJpQ"
      },
      "source": [
        "# avg_n_prior_order = np.mean(train_X.groupby(['train_order_id', 'product_id'])['prior_order_id'].count())\n",
        "# max_n_prior_order = np.max(train_X.groupby(['train_order_id', 'product_id'])['prior_order_id'].count())\n",
        "# print(f'平均每個product的prior order數: {round(avg_n_prior_order, 2)}')\n",
        "# print(f'最大每個product的prior order數: {round(max_n_prior_order, 2)}')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCvLgbyj2IFB"
      },
      "source": [
        "# OneHotEncoding & load data \n",
        "* 先fit好 encoder, 在batch training時將cat data做transform即可，降低記憶體使用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOXJo5Oo_z3g"
      },
      "source": [
        "# train_X = get_data(eval_set= 'train')\n",
        "# test_X = get_data(eval_set= 'test')\n",
        "# train_X.to_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/new_train_X.pkl')\n",
        "# test_X.to_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/new_test_X.pkl')\n",
        "train_X = pd.read_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/new_train_X.pkl')\n",
        "test_X = pd.read_pickle('/content/drive/MyDrive/python_data/kaggle/instacart/data/new_test_X.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af0pOEVeAITl"
      },
      "source": [
        "train_y = train_X['reordered']\n",
        "order_ids = test_X['order_id']\n",
        "train_X.drop(columns= ['order_id', 'user_id', 'reordered'], inplace= True)\n",
        "test_X.drop(columns= ['order_id', 'user_id'], inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DczSgnyMCTgm"
      },
      "source": [
        "'''轉小時成類別'''\n",
        "train_X['order_hour_of_day_now'] = train_X['order_hour_of_day_now'].apply(lambda x: hour2cat(x))\n",
        "test_X['order_hour_of_day_now'] = test_X['order_hour_of_day_now'].apply(lambda x: hour2cat(x))\n",
        "train_X['order_hod_mode'] = train_X['order_hod_mode'].apply(lambda x: hour2cat(x))\n",
        "test_X['order_hod_mode'] = test_X['order_hod_mode'].apply(lambda x: hour2cat(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfzDZoqVA1j1"
      },
      "source": [
        "dense_features = ['order_number_now', \n",
        "        'days_since_prior_order_now',\n",
        "        'add2cart_mode',\n",
        "        'order_number_max',\n",
        "        'days_since_mean',\n",
        "        'days_since_std',\n",
        "        'order_count',\n",
        "        'user_dep_ratio',\n",
        "        'user_aisle_ratio']\n",
        "\n",
        "train_dense, test_dense = train_X[dense_features], test_X[dense_features]\n",
        "train_cat, test_cat = train_X.drop(columns= dense_features), test_X.drop(columns= dense_features)\n",
        "\n",
        "'''標準化連續特徵'''\n",
        "scaler = MinMaxScaler()\n",
        "train_size = len(train_dense)\n",
        "dense = pd.concat([train_dense, test_dense], axis= 0)\n",
        "dense = scaler.fit_transform(dense)\n",
        "train_dense, test_dense = dense[:train_size, :], dense[train_size:, :] \n",
        "\n",
        "'''OneHotEncoder類別特徵'''\n",
        "enc = OneHotEncoder(sparse= False)\n",
        "enc_fitted = enc.fit(train_cat)\n",
        "\n",
        "'''from dataframe to numpy array'''\n",
        "train_cat, test_cat = train_cat.to_numpy(), test_cat.to_numpy()\n",
        "\n",
        "'''fields for model'''\n",
        "fields = [len(np.unique(train_cat[:, i])) for i in range(train_cat.shape[1])] + [len(dense_features)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue3WQ17bE4P8"
      },
      "source": [
        "print(len(train_cat), len(test_cat))\n",
        "# enc_fitted.transform(train_cat)\n",
        "train_y.value_counts()/len(train_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZqelqsdCuAa"
      },
      "source": [
        "# Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qap9YHPSCwYL"
      },
      "source": [
        "class custom_dataset(Dataset):\n",
        "  def __init__(self, X_cat, X_dense, y= None, if_y= False):\n",
        "    self.X_cat = torch.tensor(X_cat, dtype= torch.float)\n",
        "    self.X_dense = torch.tensor(X_dense, dtype= torch.float)\n",
        "    self.if_y= if_y\n",
        "    if if_y:\n",
        "      self.y = torch.tensor(y, dtype= torch.float)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.X_cat)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if self.if_y:\n",
        "      return self.X_cat[idx], self.X_dense[idx], self.y[idx]\n",
        "    else:\n",
        "      return self.X_cat[idx], self.X_dense[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_e_fz12FcIf"
      },
      "source": [
        "# Model \n",
        "* Evaluation: mean F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLRi_-S9FcPY"
      },
      "source": [
        "class DeepFM(nn.Module):\n",
        "  def __init__(self, fields, k= 5, hidden_dims= [16, 16], dropout= 0.2, n_class= 1):\n",
        "    super(DeepFM, self).__init__()\n",
        "    self.fields = fields \n",
        "    self.k = k \n",
        "    self.hidden_dims = hidden_dims\n",
        "    self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "    \"\"\"Linear\"\"\"\n",
        "    d = sum(fields)\n",
        "    self.linear = nn.Linear(d, n_class, bias= False)\n",
        "\n",
        "    \"\"\"FM\"\"\"\n",
        "    # self.FM_w = nn.Linear(1, n_class)\n",
        "    self.embedding_ws = nn.ModuleList()\n",
        "    for i in fields:\n",
        "      self.embedding_ws.append(nn.Linear(i, k, bias= False))\n",
        "    \n",
        "    \"\"\"DNN\"\"\"\n",
        "    layers = []\n",
        "    input_dim = k * len(fields)\n",
        "\n",
        "    for hidden_dim in hidden_dims:\n",
        "      layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "      layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(self.dropout)\n",
        "      input_dim = hidden_dim\n",
        "    \n",
        "    layers.append(nn.Linear(hidden_dims[-1], n_class))\n",
        "    self.dnn = nn.Sequential(*layers)\n",
        "\n",
        "  def Dense_Embedding(self, X):\n",
        "    es = []\n",
        "    start= 0\n",
        "    for i, field in enumerate(self.fields):\n",
        "      ei = self.embedding_ws[i](X[:, start:start+field]).unsqueeze(dim= 1) # ei: [n, 1, k]\n",
        "      # ei = torch.matmul(X[:, start:start+field], self.embedding_ws[i]).unsqueeze(dim= 1) # ei: [n, 1, k]\n",
        "      start += field\n",
        "      es.append(ei)\n",
        "\n",
        "    return torch.cat(es, dim= 1) # [n, n_fields, k]  \n",
        "\n",
        "  \n",
        "  def FM(self, X):\n",
        "\n",
        "    sum_of_square = torch.sum(X, dim= 1)**2 #[n, k]\n",
        "    square_of_sum = torch.sum(X**2, dim= 1)\n",
        "    ix = sum_of_square - square_of_sum \n",
        "    FM_out = 0.5 * torch.sum(ix, dim= 1, keepdim= True) # [n, 1] \n",
        "    FM_out = self.dropout(FM_out)\n",
        "    # return self.FM_w(FM_out)\n",
        "    return FM_out\n",
        "\n",
        "  def DNN(self, X):\n",
        "\n",
        "    X = X.view(-1, self.k * len(self.fields)) # [n, k*n_fields]\n",
        "    X = self.dnn(X)\n",
        "    return X\n",
        "  \n",
        "  def forward(self, X):\n",
        "\n",
        "    dense_X = self.Dense_Embedding(X)\n",
        "    FM_y = self.FM(dense_X)\n",
        "    DNN_y = self.DNN(dense_X)\n",
        "    y = self.dropout(self.linear(X)) + FM_y + DNN_y\n",
        "\n",
        "    # return nn.Sigmoid()(y) # BCELoss\n",
        "    return y # nn.BCEWithLogitsLoss(pos_weight=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EWbHL5t60e_"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZI3eM2c61TI"
      },
      "source": [
        "batch_size= 512\n",
        "lr = 1e-3\n",
        "n_epoch = 2\n",
        "k = 10\n",
        "p = 0.5\n",
        "hidden_dims = [64, 64]\n",
        "n_class = 1\n",
        "threshold= 0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvS3EOzG65ci"
      },
      "source": [
        "train_dataset = custom_dataset(train_cat, train_dense, train_y, if_y= True)\n",
        "# train_size= int(0.8*len(data_dataset))\n",
        "# val_size= len(data_dataset)- train_size\n",
        "\n",
        "'''train, val'''\n",
        "# train_dataset, val_dataset= random_split(data_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle= True, num_workers=2)\n",
        "# val_loader = DataLoader(val_dataset, batch_size= batch_size, shuffle= False, num_workers=2)\n",
        "\n",
        "'''test'''\n",
        "test_dataset = custom_dataset(test_cat, test_dense, if_y= False)\n",
        "test_loader = DataLoader(test_dataset, batch_size= 512, shuffle= False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgGvPoz365ed"
      },
      "source": [
        "model= DeepFM(fields= fields, k= k, hidden_dims= hidden_dims, dropout= p, n_class= n_class).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "criterion= nn.BCEWithLogitsLoss()\n",
        "# criterion= nn.BCEWithLogitsLoss(pos_weight=torch.tensor(9, device= device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M808Am5V7CoC"
      },
      "source": [
        "## Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "139lTdHJ65gJ"
      },
      "source": [
        "for epoch in range(n_epoch):\n",
        "\n",
        "  model.train()\n",
        "  train_loss= 0\n",
        "  train_score= 0\n",
        "  val_score= 0\n",
        "\n",
        "  '''train'''\n",
        "  for i, (X_cat, X_dense, y) in enumerate(tqdm(train_loader)):\n",
        "  # for i, (X_cat, X_dense, y) in enumerate(train_loader):\n",
        "    X_cat_onehot = torch.tensor(enc_fitted.transform(X_cat), dtype= torch.float)\n",
        "    X= torch.cat([X_cat_onehot, X_dense], dim= 1).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output= model(X)\n",
        "    loss= criterion(output, y.unsqueeze(dim= 1).to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    train_score += sum(nn.Sigmoid()(output)>threshold)\n",
        "\n",
        "  print(f'train loss: {round(train_loss.item()/train_size, 3)}| train accu: {round(train_score/train_size, 3)}')\n",
        "\n",
        "  # '''val'''\n",
        "  # model.eval()\n",
        "  # for i, (X_cat, X_dense, y) in enumerate(tqdm(val_loader)):\n",
        "  #   X_cat_onehot = torch.tensor(enc_fitted.transform(X_cat), dtype= torch.float)\n",
        "  #   X= torch.cat([X_cat_onehot, X_dense], dim= 1).to(device)\n",
        "  #   with torch.no_grad():\n",
        "  #     output= model(X)\n",
        "  #   val_score += sum(nn.Sigmoid()(output)>threshold)\n",
        "  # print(f'val accu: {round(val_score.item()/val_size, 3)}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4U8q4jB65iG"
      },
      "source": [
        "# torch.save(model, f'instacart.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq8SB0Akr6_C"
      },
      "source": [
        "# !cp 'instacart.pt' '/content/drive/MyDrive/python_data/kaggle/instacart'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnrnaypUyodD"
      },
      "source": [
        "# Create Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNf20I4PuoKx"
      },
      "source": [
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oL_TTXSJhpZ"
      },
      "source": [
        "model= torch.load('/content/drive/MyDrive/python_data/kaggle/instacart/instacart.pt', map_location=torch.device(device))\n",
        "# model= torch.load('/content/drive/MyDrive/python_data/kaggle/instacart/instacart.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUpmJ6O5yrNO"
      },
      "source": [
        "'''\n",
        "if nn.BCEWithLogitsLoss(pos_weight=9) is used, \n",
        "then output from the model should pass a sigmoid function to represent probability for test data\n",
        "'''\n",
        "# import time \n",
        "\n",
        "preds= []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i, (X_cat, X_dense) in enumerate(tqdm(test_loader)):\n",
        "    # st= time.time()\n",
        "    X_cat_onehot = torch.tensor(enc_fitted.transform(X_cat), dtype= torch.float)\n",
        "    X= torch.cat([X_cat_onehot, X_dense], dim= 1).to(device)\n",
        "    # en= time.time()\n",
        "    # print(f'time taken: {en-st}s')\n",
        "    output= model(X)\n",
        "    output= nn.Sigmoid()(output) # Careful \n",
        "    preds.extend(output.squeeze(dim=1).detach().cpu().numpy())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1CHnkT90O_w"
      },
      "source": [
        "preds = np.array(preds)\n",
        "np.save('/content/drive/MyDrive/python_data/kaggle/instacart/data/preds.npy', preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aFLlA1Wi82L"
      },
      "source": [
        "preds= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/preds.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vywKqTPrExas"
      },
      "source": [
        "test_X['order_id']= test_order_id\n",
        "test_X['pred']= preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JCGTUFvWhSQ"
      },
      "source": [
        "# (test_X_cat[:, 2] == -1).any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrNzK87sjL-s"
      },
      "source": [
        "def rule(x):\n",
        "  if x >= 0.6:\n",
        "    return 1 \n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZmJ7eDxWixC"
      },
      "source": [
        "test_X['pred_binary'] = test_X['pred'].apply(rule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHXpoWvri1gx"
      },
      "source": [
        "test_X['pred_binary'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITgeydR5jVTh"
      },
      "source": [
        "submission_dict= {}\n",
        "for i, row in test_X[test_X['pred_binary']==1].iterrows():\n",
        "  order_id = int(row['order_id'])\n",
        "  product_id= int(row['product_id'])\n",
        "  if order_id in submission_dict.keys():\n",
        "    submission_dict[order_id].append(product_id)\n",
        "  else:\n",
        "    submission_dict[order_id] = [product_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcYMf2V5ljNL"
      },
      "source": [
        "for order_id in test_X[test_X['pred_binary']==0]['order_id'].unique():\n",
        "  if order_id in submission_dict.keys():\n",
        "    pass\n",
        "  else:\n",
        "    submission_dict[order_id]= 'None'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGZHxAb0prPZ"
      },
      "source": [
        "submission_dict.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH8NeWnSvLNo"
      },
      "source": [
        "with open('submission.csv', 'w', newline='') as csvfile:\n",
        "  # 建立 CSV 檔寫入器\n",
        "  writer = csv.writer(csvfile, delimiter=',')\n",
        "\n",
        "  # 寫入一列資料\n",
        "  writer.writerow(['order_id', 'products'])\n",
        "\n",
        "  # 寫入另外幾列資料\n",
        "  for key, value in submission_dict.items():\n",
        "    if value == 'None':\n",
        "      writer.writerow([key, 'None'])\n",
        "    else:\n",
        "      value= [str(id) for id in value]\n",
        "      writer.writerow([str(key), ' '.join(value)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSVDIkS3wM_Y"
      },
      "source": [
        "len(submission_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPBYhY3JPYlh"
      },
      "source": [
        "# Trivial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqs3o2ctjOsk"
      },
      "source": [
        "! pip install pyarrow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry0Tba7njShs"
      },
      "source": [
        "import pyarrow.parquet as pq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbOR-tFfOleQ"
      },
      "source": [
        "def load_data(from_file= True):\n",
        "\tif from_file:\n",
        "\t\t'''load parquet from data'''\n",
        "\t\t# train_X1= pd.read_parquet('./data/train_X1.parquet', engine='pyarrow')\n",
        "\t\t# train_X2= pd.read_parquet('./data/train_X2.parquet', engine='pyarrow')\n",
        "\t\t# test_X= pd.read_parquet('./data/test_X.parquet', engine='pyarrow')\n",
        "\t\t\n",
        "\t\ttrain_y= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_y.npy') # allow_pickle=True Series\n",
        "\t\ttrain_X1= pq.read_table('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_X1.parquet').to_pandas()\n",
        "\t\ttrain_X2= pq.read_table('/content/drive/MyDrive/python_data/kaggle/instacart/data/train_X2.parquet').to_pandas()\n",
        "\t\ttest_X= pq.read_table('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_X.parquet').to_pandas()\n",
        "\t\ttest_order_id= np.load('/content/drive/MyDrive/python_data/kaggle/instacart/data/test_order_id.npy')\n",
        "\t\ttrain_X= pd.concat([train_X1, train_X2], axis= 0)\n",
        "\telse:\n",
        "\t\ttrain_X, train_y= get_data(train_bool= True)\n",
        "\t\ttest_X, test_order_id= get_data(train_bool= False)\n",
        "\treturn train_X, train_y, test_X, test_order_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTokUmd8O9xQ"
      },
      "source": [
        "evice= 'cpu'\n",
        "print(f'using {device}')\n",
        "\n",
        "train_X, train_y, test_X, test_order_id= load_data(from_file= True)\n",
        "print('Data Loaded!!!')\n",
        "\n",
        "train_dense_cols= ['train_days_since_prior_order', 'prior_order_count', 'prior_dspo_mean', 'prior_dspo_var', 'user_dep_ratio', 'user_aisle_ratio']\n",
        "test_dense_cols= ['test_days_since_prior_order', 'prior_order_count', 'prior_dspo_mean', 'prior_dspo_var', 'user_dep_ratio', 'user_aisle_ratio']\n",
        "\n",
        "train_X_cat= train_X.drop(train_dense_cols, axis= 1).to_numpy()\n",
        "train_X_dense= train_X[train_dense_cols].to_numpy()\n",
        "# train_y= train_y.to_numpy()\n",
        "\n",
        "test_X_cat= test_X.drop(test_dense_cols, axis= 1).to_numpy()\n",
        "test_X_dense= test_X[test_dense_cols].to_numpy()\n",
        "\n",
        "fields = [len(np.unique(train_X_cat[:, i])) for i in range(train_X_cat.shape[1])] + [len(train_dense_cols)]\n",
        "\n",
        "encoder= OneHotEncoder(sparse= False, handle_unknown='ignore')\n",
        "enc_fitted= encoder.fit(train_X_cat)\n",
        "\n",
        "'''test dataloader'''\n",
        "test_dataset = custom_dataset(test_X_cat, test_X_dense, if_y= False)\n",
        "test_loader = DataLoader(test_dataset, batch_size= 512, shuffle= False, num_workers=2)\n",
        "\n",
        "\"\"\"load model from pt file\"\"\"\n",
        "print('Loading model...')\n",
        "model= torch.load('/content/drive/MyDrive/python_data/kaggle/instacart/instacart.pt', map_location=torch.device(device))\n",
        "\n",
        "\n",
        "\"\"\"Testing Phase\"\"\"\n",
        "print('start test phase...')\n",
        "preds= []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\tfor i, (X_cat, X_dense) in enumerate(tqdm(test_loader)):\n",
        "\t\tX_cat_onehot = torch.tensor(enc_fitted.transform(X_cat), dtype= torch.float)\n",
        "\t\tX= torch.cat([X_cat_onehot, X_dense], dim= 1).to(device)\n",
        "\t\toutput= model(X)\n",
        "\t\toutput= nn.Sigmoid()(output) # Careful \n",
        "\t\tpreds.extend(output.squeeze(dim=1).detach().cpu().numpy())  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}